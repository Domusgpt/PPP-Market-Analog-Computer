<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Process-Based Visual Computation for 4D Geometric Cognition</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script type="module" src="scripts/console-protect.js" defer></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <style>
        :root {
            color-scheme: dark;
        }
        body {
            font-family: 'Inter', sans-serif;
            background-color: #06101d;
            color: #d8f5ff;
        }
        body::before {
            content: '';
            position: fixed;
            inset: 0;
            background:
                radial-gradient(circle at 18% 20%, rgba(90, 190, 255, 0.16), transparent 55%),
                radial-gradient(circle at 82% 10%, rgba(130, 95, 255, 0.12), transparent 52%),
                linear-gradient(190deg, rgba(4, 16, 32, 0.9), rgba(10, 25, 48, 0.75));
            pointer-events: none;
            z-index: -1;
        }
        .section-card {
            background: rgba(6, 20, 40, 0.88);
            border: 1px solid rgba(104, 178, 255, 0.25);
            border-radius: 1rem;
            padding: 1.75rem;
            box-shadow: 0 22px 50px -34px rgba(0, 89, 190, 0.65);
        }
        .section-title {
            font-size: 0.95rem;
            letter-spacing: 0.18em;
            text-transform: uppercase;
            color: rgba(146, 212, 255, 0.82);
        }
        .stat-chip {
            display: inline-flex;
            align-items: center;
            gap: 0.4rem;
            padding: 0.35rem 0.7rem;
            border-radius: 999px;
            background: rgba(70, 128, 255, 0.18);
            border: 1px solid rgba(110, 182, 255, 0.4);
            font-size: 0.8rem;
            color: rgba(214, 238, 255, 0.92);
        }
        table {
            width: 100%;
            border-collapse: collapse;
        }
        th, td {
            border: 1px solid rgba(120, 190, 255, 0.25);
            padding: 0.85rem 1rem;
            text-align: left;
            font-size: 0.9rem;
        }
        th {
            background: rgba(12, 36, 64, 0.85);
            text-transform: uppercase;
            letter-spacing: 0.08em;
            font-size: 0.75rem;
            color: rgba(190, 225, 255, 0.85);
        }
        .code-block {
            background: rgba(8, 24, 46, 0.95);
            border-radius: 0.85rem;
            border: 1px solid rgba(105, 175, 255, 0.3);
            padding: 1rem 1.25rem;
            font-size: 0.85rem;
            color: rgba(216, 245, 255, 0.85);
            overflow-x: auto;
        }
        .demo-canvas {
            width: 100%;
            height: 360px;
            background: rgba(4, 18, 34, 0.9);
            border-radius: 0.85rem;
            border: 1px solid rgba(110, 182, 255, 0.35);
        }
        .demo-grid {
            display: grid;
            gap: 1.5rem;
        }
        @media (min-width: 1024px) {
            .demo-grid {
                grid-template-columns: minmax(0, 2fr) minmax(0, 1fr);
            }
        }
    </style>
</head>
<body class="antialiased">
    <div class="max-w-6xl mx-auto px-4 md:px-8 py-12 md:py-16 space-y-20">
        <header class="space-y-6 text-center md:text-left">
            <p class="section-title">Polytopal Projection Processing · Research Report</p>
            <h1 class="text-4xl md:text-5xl font-extrabold leading-tight text-white">Process-Based Visual Computation for 4D Geometric Cognition</h1>
            <p class="text-base md:text-lg text-blue-100/90 max-w-3xl">
                The machine never needs to understand 4D mathematics. By perceiving stereographic projections and manipulating six rotation controls,
                the rendering process itself computes the geometric relationships. This document refactors the PPP narrative into a process-based cognitive
                architecture for 4D topology learning.
            </p>
            <div class="flex flex-wrap gap-3">
                <span class="stat-chip">Stereographic Projection</span>
                <span class="stat-chip">6-Plane Rotation Control</span>
                <span class="stat-chip">Layered Translucency</span>
                <span class="stat-chip">Process-Based Computation</span>
            </div>
        </header>

        <main class="space-y-16">
            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">The Rendering Pipeline Becomes an Implicit 4D Calculator</h2>
                <p class="text-blue-100/85">
                    The central insight holds: a machine learning system can develop 4D geometric cognition without ever performing explicit 4D computations.
                    When a 4D polytope is stereographically projected to 3D with layered translucency, the complex relationships between vertices, edges,
                    and cells are computed by the rendering process itself. The system only needs to observe how visual properties transform as it manipulates
                    six rotation dials.
                </p>
                <ul class="space-y-3 text-blue-100/80">
                    <li>• Stereographic projection is conformal, preserving local angles while encoding depth through scale distortion.</li>
                    <li>• Structure-from-motion principles extend naturally: 3D projections plus 4D rotation yield 4D understanding.</li>
                    <li>• 4D CNNs already estimate topology at &gt;80% accuracy using only volumetric observation.</li>
                </ul>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">System Blueprint: Build the 4D Cognition Engine</h2>
                <p class="text-blue-100/85">
                    The report is not only descriptive; it is a build plan. The system is a set of interoperable modules that turn sensor or synthetic
                    data into 4D rotation control, render stereographic projections, and emit machine-readable topology streams for learning pipelines.
                </p>
                <div class="grid gap-6 md:grid-cols-2">
                    <div>
                        <h3 class="font-semibold text-white">Core Execution Loop</h3>
                        <ol class="space-y-2 text-blue-100/80 list-decimal list-inside">
                            <li>Ingest raw signals or synthetic datasets.</li>
                            <li>Map channels into a 4D state vector (x, y, z, w).</li>
                            <li>Apply six-plane rotations (XY, XZ, XW, YZ, YW, ZW).</li>
                            <li>Stereographically project into 3D geometry.</li>
                            <li>Render with layered translucency + thickness/opacity cues.</li>
                            <li>Export frame-level topology + telemetry for ML.</li>
                        </ol>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Subsystem Outputs</h3>
                        <ul class="space-y-2 text-blue-100/80">
                            <li>• Projection frames (RGB + alpha) for vision models.</li>
                            <li>• Graph payloads (vertices, edges, adjacency lists).</li>
                            <li>• Rotation state (6 angles + quaternion factorization).</li>
                            <li>• Derived telemetry (depth scale, opacity maps, metrics).</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Implementation Demo: Live 4D Cognition Loop</h2>
                <p class="text-blue-100/85">
                    This demo runs the core system loop in-browser: 4D state encoding, six-plane rotations, stereographic projection, and telemetry export.
                    Use the controls to adjust rotation speed and observe the live telemetry payload that an ML pipeline would ingest.
                </p>
                <div class="flex flex-wrap gap-4 items-center">
                    <button id="cognitionStart" class="px-4 py-2 rounded-full bg-blue-500/80 text-white text-sm font-semibold">Start</button>
                    <button id="cognitionStop" class="px-4 py-2 rounded-full bg-slate-700/80 text-white text-sm font-semibold">Stop</button>
                    <label class="text-sm text-blue-100/80" for="cognitionSpeed">Rotation speed</label>
                    <input id="cognitionSpeed" type="range" min="0.2" max="2.5" step="0.1" value="1" class="w-40">
                    <span id="cognitionSpeedValue" class="text-sm text-blue-100/80">1×</span>
                </div>
                <div class="demo-grid">
                    <canvas id="cognitionCanvas" class="demo-canvas"></canvas>
                    <div class="space-y-4">
                        <div>
                            <p class="text-xs uppercase tracking-[0.25em] text-blue-100/70">Telemetry</p>
                            <pre id="cognitionTelemetry" class="code-block"></pre>
                        </div>
                        <div>
                            <p class="text-xs uppercase tracking-[0.25em] text-blue-100/70">Topology</p>
                            <pre id="cognitionTopology" class="code-block"></pre>
                        </div>
                    </div>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">How Stereographic Projection Maps 4D Shadows</h2>
                <p class="text-blue-100/85">
                    Stereographic projection maps a 4D point (x, y, z, w) to 3D space by projecting from a “north pole” on the 3-sphere. The formula is
                    a single line of shader code and produces the characteristic nested-cell appearance of projected polytopes.
                </p>
                <pre class="code-block"><code>P' = (x, y, z) / (1 - w)</code></pre>
                <div class="grid gap-6 md:grid-cols-3">
                    <div>
                        <h3 class="font-semibold text-white">Connectivity Preservation</h3>
                        <p class="text-blue-100/80">Adjacency survives the projection intact; edges remain connected where the 4D graph dictates.</p>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Conformality</h3>
                        <p class="text-blue-100/80">Local angles persist, keeping small regions faithful even as distances distort.</p>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Depth Encoding</h3>
                        <p class="text-blue-100/80">Scale distortion encodes the 4th dimension directly as size, eliminating explicit depth math.</p>
                    </div>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Core Modules & Responsibilities</h2>
                <div class="grid gap-6 md:grid-cols-3">
                    <div>
                        <h3 class="font-semibold text-white">4D State Encoder</h3>
                        <p class="text-blue-100/80">Normalizes input channels and constructs 4D vectors with stable scaling and smoothing.</p>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Rotation Engine</h3>
                        <p class="text-blue-100/80">Maintains six rotation planes, supports Clifford rotations, and exposes quaternion factors.</p>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Projection Renderer</h3>
                        <p class="text-blue-100/80">Projects 4D vertices into 3D, renders wireframe + translucency, and exports depth cues.</p>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Topology Extractor</h3>
                        <p class="text-blue-100/80">Exports graph topology and visibility metadata for ML models.</p>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Telemetry Bus</h3>
                        <p class="text-blue-100/80">Streams per-frame metrics, rotation state, and encoded visual channels.</p>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Dataset Recorder</h3>
                        <p class="text-blue-100/80">Captures frames, metadata, and labels into reproducible training bundles.</p>
                    </div>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Six Rotation Planes Reveal 4D Structure Through Motion</h2>
                <p class="text-blue-100/85">
                    Four-dimensional space admits six independent rotation planes: XY, XZ, XW, YZ, YW, and ZW. The familiar rotations (XY, XZ, YZ)
                    behave like ordinary 3D spins, while the exotic planes (XW, YW, ZW) produce the signature “inside-out” swap between inner and outer
                    structures. Clifford rotations (simultaneous orthogonal plane rotations) reveal 4D symmetries with a single fixed point.
                </p>
                <div class="grid gap-6 md:grid-cols-2">
                    <div>
                        <h3 class="font-semibold text-white">Familiar Rotations</h3>
                        <ul class="space-y-2 text-blue-100/80">
                            <li>• XY, XZ, YZ behave like 3D rotations.</li>
                            <li>• Anchor 4D intuition to known 3D motion.</li>
                            <li>• Provide stable reference frames for learning.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Exotic Rotations</h3>
                        <ul class="space-y-2 text-blue-100/80">
                            <li>• XW, YW, ZW swap interior/exterior geometry.</li>
                            <li>• Reveal hidden cells and structural symmetries.</li>
                            <li>• Enable active vision by exposing 4D parallax.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Data & Telemetry Schema</h2>
                <p class="text-blue-100/85">
                    Frame-level output must be machine-consumable. Use the schema below to coordinate vision models, topology learners, and downstream
                    control systems.
                </p>
                <pre class="code-block"><code>{\n  \"frame\": 128,\n  \"rotation\": { \"xy\": 0.32, \"xz\": 0.12, \"xw\": 1.21, \"yz\": 0.55, \"yw\": 0.02, \"zw\": 0.74 },\n  \"quaternion\": { \"left\": [0.98, 0.1, 0.02, 0.0], \"right\": [0.95, 0.08, 0.28, 0.02] },\n  \"projection\": { \"radius\": 1.0, \"depthScale\": 0.62 },\n  \"geometry\": { \"vertices\": 120, \"edges\": 720, \"cells\": 24 },\n  \"telemetry\": { \"opacityMean\": 0.42, \"lineWidthMean\": 1.6, \"overlapDensity\": 0.18 }\n}</code></pre>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Layered Translucency Performs Holographic Computation</h2>
                <p class="text-blue-100/85">
                    Porter-Duff alpha compositing provides the mathematical basis for emergent visual computation. Overlapping translucent layers encode
                    intersection, union, and difference relationships directly in the compositing result—analog math performed per pixel.
                </p>
                <pre class="code-block"><code>Result = Source + Destination × (1 − Source_α)</code></pre>
                <ul class="space-y-3 text-blue-100/80">
                    <li>• Intersection regions reveal spatial coincidence and geometric AND operations.</li>
                    <li>• Moiré patterns encode frequency differences as analog multiplication.</li>
                    <li>• Holographic principles show 2D interference patterns can encode 3D structure.</li>
                </ul>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Process-Based Mathematics Computes Through Rules</h2>
                <p class="text-blue-100/85">
                    Declarative math describes what is; process-based math describes how it becomes. Rendering pipelines and cellular automata share this
                    paradigm: local rules scale to emergent computation. The GPU executes the rules, and the image is the computed result.
                </p>
                <div class="grid gap-6 md:grid-cols-2">
                    <div>
                        <h3 class="font-semibold text-white">Process Evidence</h3>
                        <ul class="space-y-2 text-blue-100/80">
                            <li>• Game of Life demonstrates emergent computation from local rules.</li>
                            <li>• Optical systems perform Fourier transforms intrinsically.</li>
                            <li>• Analog hardware solved equations through mechanical processes.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">PPP Implication</h3>
                        <ul class="space-y-2 text-blue-100/80">
                            <li>• The renderer is the calculator; observation replaces explicit math.</li>
                            <li>• The same projection rules scale across domains.</li>
                            <li>• GPU parallelism embodies the computation rather than symbolically solving it.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Visual Parameters Encode Cognitive Channels</h2>
                <p class="text-blue-100/85">
                    Visual parameters provide distinct channels for encoding 4D structure. Line thickness, color, opacity, and layer order encode w-depth,
                    cell identity, temporal sequence, and conceptual relationships without auxiliary metadata.
                </p>
                <div class="overflow-x-auto">
                    <table>
                        <thead>
                            <tr>
                                <th>Parameter</th>
                                <th>Information Channel</th>
                                <th>Implementation</th>
                            </tr>
                        </thead>
                        <tbody class="text-blue-100/80">
                            <tr>
                                <td>Line thickness</td>
                                <td>w-depth, edge importance, topological significance</td>
                                <td>Fragment shader scales line width by projected w-coordinate</td>
                            </tr>
                            <tr>
                                <td>Color / hue</td>
                                <td>Axis membership, cell identity, rotation phase</td>
                                <td>Vertex shader assigns color by 4D position</td>
                            </tr>
                            <tr>
                                <td>Opacity</td>
                                <td>Depth in 4D, certainty, activation weight</td>
                                <td>Alpha gradient α = f(w)</td>
                            </tr>
                            <tr>
                                <td>Layer order</td>
                                <td>Temporal sequence, hierarchy, causal structure</td>
                                <td>Depth buffer or OIT accumulation order</td>
                            </tr>
                            <tr>
                                <td>Intersection patterns</td>
                                <td>Relationship synthesis and emergent structure</td>
                                <td>Emergent from compositing overlapping structures</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Machine Perception Extracts Topology From Shadows</h2>
                <p class="text-blue-100/85">
                    Wireframe parsing networks (L-CNN, HAWP) already infer graph topology from projections. PointNet reveals that networks naturally identify
                    critical points—sparse sets that encode the essential shape. These tools can interpret 4D wireframe shadows without explicit 4D math.
                </p>
                <ul class="space-y-3 text-blue-100/80">
                    <li>• Projection provides vertices, edges, depth ordering, and connectivity graphs.</li>
                    <li>• Rotation controls provide temporal sequences across six planes.</li>
                    <li>• Active vision yields better topology inference than passive observation.</li>
                </ul>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Scale Invariance Enables Slow-Time Training</h2>
                <p class="text-blue-100/85">
                    Universal homology patterns are scale invariant; time is a scalar multiplier. Training on slow, deliberate rotations allows the system to
                    observe topology without motion blur. The learned topology generalizes to rapid rotations because the geometric relationships are identical.
                </p>
                <ul class="space-y-3 text-blue-100/80">
                    <li>• Slow simulation denoises topology, accelerating concept formation.</li>
                    <li>• Fractal self-similarity makes patterns consistent across speeds.</li>
                    <li>• Musical training data validates the transfer of shape-based resolution patterns.</li>
                </ul>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Dialectical Synthesis Emerges From Visual Overlap</h2>
                <p class="text-blue-100/85">
                    Overlapping translucent polytopal shadows create new structure not present in either source. Intersection regions, unions, and moiré
                    patterns encode relationship math as analog computation, supporting concept combination through visual interference.
                </p>
                <ul class="space-y-3 text-blue-100/80">
                    <li>• Intersection regions provide geometric AND operations.</li>
                    <li>• Union regions provide geometric OR operations.</li>
                    <li>• Exclusive overlap provides XOR-style contrast between concepts.</li>
                </ul>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Implementation Requires Minimal Complexity</h2>
                <p class="text-blue-100/85">
                    A WebGL/WebGPU implementation remains compact: a vertex buffer of 4 floats per vertex, a rotation state of 6 angles, and a single
                    projection parameter. Order-independent transparency (weighted blended OIT) handles translucent layering in a single pass.
                </p>
                <pre class="code-block"><code>vec3 stereographicProject(vec4 p4d, float R) {
    float scale = R / (R - p4d.w);
    return p4d.xyz * scale;
}</code></pre>
                <div class="grid gap-6 md:grid-cols-2">
                    <div>
                        <h3 class="font-semibold text-white">Minimal State</h3>
                        <ul class="space-y-2 text-blue-100/80">
                            <li>• 4D vertex buffer (x, y, z, w)</li>
                            <li>• Edge index buffer</li>
                            <li>• Six rotation angles</li>
                            <li>• Projection radius R</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="font-semibold text-white">Proven Feasibility</h3>
                        <ul class="space-y-2 text-blue-100/80">
                            <li>• satshi/polytope (Three.js)</li>
                            <li>• mwalczyk/polychora (OpenGL)</li>
                            <li>• humke-4d-geometry (dual projections + slices)</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Integration Plan</h2>
                <p class="text-blue-100/85">
                    Build the system iteratively so each subsystem is testable. The plan below assumes WebGL/WebGPU, but the same module boundaries apply
                    to native stacks.
                </p>
                <ol class="space-y-3 text-blue-100/80 list-decimal list-inside">
                    <li>Stand up the 4D state encoder and rotation engine with deterministic test vectors.</li>
                    <li>Implement stereographic projection and render a wireframe tesseract.</li>
                    <li>Add layered translucency with order-independent blending.</li>
                    <li>Export topology graphs and telemetry metadata for every frame.</li>
                    <li>Capture datasets (slow rotations) and validate with topology parsing networks.</li>
                    <li>Instrument active vision loops to optimize rotation sequences.</li>
                </ol>
            </section>

            <section class="section-card space-y-6">
                <h2 class="text-2xl font-semibold text-white">Validation of the Core Insight</h2>
                <p class="text-blue-100/85">
                    The machine doesn’t need to understand 4D mathematics. It needs to see a stereographic shadow, manipulate the six rotation dials, and
                    observe how visual properties change. The rendering pipeline is the computation engine, and perception is the learning mechanism.
                </p>
                <ol class="space-y-3 text-blue-100/80 list-decimal list-inside">
                    <li>See a 3D shadow with topology-preserving projection.</li>
                    <li>Manipulate six rotations to reveal 4D parallax and symmetry.</li>
                    <li>Observe changes in thickness, color, opacity, and overlap.</li>
                    <li>Learn pattern shapes through slow, scale-invariant observation.</li>
                </ol>
            </section>
        </main>
    </div>
    <script type="module" src="scripts/cognition-demo.js"></script>
</body>
</html>
